{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4411abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "from numpy.fft import fft, fftshift\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbd8c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7d3ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "# path 변수를 적절히 변경\n",
    "x_train_path = os.path.join('data/train_features.csv')\n",
    "y_train_path = os.path.join('data/train_labels.csv')\n",
    "x_test_path = os.path.join('data/test_features.csv')\n",
    "sub_path = os.path.join('data/sample_submission.csv')\n",
    "\n",
    "train = pd.read_csv(x_train_path)\n",
    "train_label = pd.read_csv(y_train_path)\n",
    "test = pd.read_csv(x_test_path)\n",
    "sub = pd.read_csv(sub_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c08c5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['acc_Energy']=(train['acc_x']**2+train['acc_y']**2+train['acc_z']**2)**(1/3)\n",
    "test['acc_Energy']=(test['acc_x']**2+test['acc_y']**2+test['acc_z']**2)**(1/3)\n",
    "\n",
    "train['gy_Energy']=(train['gy_x']**2+train['gy_y']**2+train['gy_z']**2)**(1/3)\n",
    "test['gy_Energy']=(test['gy_x']**2+test['gy_y']**2+test['gy_z']**2)**(1/3)\n",
    "\n",
    "train['gy_acc_Energy']=((train['gy_x']-train['acc_x'])**2+(train['gy_y']-train['acc_y'])**2+(train['gy_z']-train['acc_z'])**2)**(1/3)\n",
    "test['gy_acc_Energy']=((test['gy_x']-test['acc_x'])**2+(test['gy_y']-test['acc_y'])**2+(test['gy_z']-test['acc_z'])**2)**(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe20beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=0.02 \n",
    "def jerk_signal(signal): \n",
    "        return np.array([(signal[i+1]-signal[i])/dt for i in range(len(signal)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c50eb30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:27<00:00, 111.75it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dt=[]\n",
    "for i in tqdm(train['id'].unique()):\n",
    "    temp=train.loc[train['id']==i]\n",
    "    for v in train.columns[2:]:\n",
    "        values=jerk_signal(temp[v].values)\n",
    "        values=np.insert(values,0,0)\n",
    "        temp.loc[:,v+'_dt']=values\n",
    "    train_dt.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edfb8794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:06<00:00, 127.31it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dt=[]\n",
    "for i in tqdm(test['id'].unique()):\n",
    "    temp=test.loc[test['id']==i]\n",
    "    for v in train.columns[2:]:\n",
    "        values=jerk_signal(temp[v].values)\n",
    "        values=np.insert(values,0,0)\n",
    "        temp.loc[:,v+'_dt']=values\n",
    "    test_dt.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cad7cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import fftpack\n",
    "from numpy.fft import *\n",
    "\n",
    "def fourier_transform_one_signal(t_signal):\n",
    "    complex_f_signal= fftpack.fft(t_signal)\n",
    "    amplitude_f_signal=np.abs(complex_f_signal)\n",
    "    return amplitude_f_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "181a360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.concat(train_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13a92345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:09<00:00, 323.44it/s]\n"
     ]
    }
   ],
   "source": [
    "fft=[]\n",
    "for i in tqdm(train['id'].unique()):\n",
    "    temp=train.loc[train['id']==i]\n",
    "    for i in train.columns[2:8]:\n",
    "        temp[i]=fourier_transform_one_signal(temp[i].values)\n",
    "    fft.append(temp)\n",
    "train=pd.concat(fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e9e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.concat(test_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9618f574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:01<00:00, 532.89it/s]\n"
     ]
    }
   ],
   "source": [
    "fft_t=[]\n",
    "for i in tqdm(test['id'].unique()):\n",
    "    temp=test.loc[test['id']==i]\n",
    "    for i in test.columns[2:8]:\n",
    "        temp[i]=fourier_transform_one_signal(temp[i].values)\n",
    "    fft_t.append(temp)\n",
    "test=pd.concat(fft_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea65ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "col=train.columns\n",
    "train_s=train.copy()\n",
    "test_s=test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "542bacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "train_s.iloc[:,2:]= scaler.fit_transform(train_s.iloc[:,2:])\n",
    "train_sc = pd.DataFrame(data = train_s,columns =col)\n",
    "\n",
    "test_s.iloc[:,2:]= scaler.transform(test_s.iloc[:,2:])\n",
    "test_sc = pd.DataFrame(data = test_s,columns =col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56a692ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset\n",
    "# x를 시계열 데이터로 변경 시켜주는 함수\n",
    "def make_series(data):\n",
    "    ids = data['id'].unique()\n",
    "    id_data = data.groupby('id')\n",
    "    series_data = []\n",
    "\n",
    "    for i in ids:\n",
    "        df = id_data.get_group(i)\n",
    "        df = df.drop(['id', 'time'], axis=1)\n",
    "        series_data.append(df.to_numpy())\n",
    "\n",
    "    series_data = np.array(series_data)\n",
    "    return series_data\n",
    "\n",
    "\n",
    "def ids_26(label):\n",
    "    mask = label['label'] == 26\n",
    "    ids = label.loc[mask, 'id'].tolist()\n",
    "    \n",
    "    return ids\n",
    "\n",
    "\n",
    "def make_split_dataset(series_train, ids, labels):\n",
    "    final_list = []\n",
    "    \n",
    "    k_split = KFold(n_splits=15, shuffle=True, random_state=42)\n",
    "    \n",
    "    temp = np.array(range(3125))\n",
    "    except_train_mask = np.setdiff1d(temp, ids, assume_unique=True)\n",
    "    except_train = series_train[except_train_mask]\n",
    "    except_label = labels[except_train_mask]\n",
    "    \n",
    "    train_26 = series_train[ids]\n",
    "\n",
    "    for _, fold in k_split.split(train_26):\n",
    "        temp_train = train_26[fold]\n",
    "        temp_label = np.array([26] * len(temp_train))\n",
    "        \n",
    "        temp_train = np.concatenate([temp_train, except_train], axis=0)\n",
    "        temp_label = np.concatenate([temp_label, except_label], axis=0)\n",
    "        print(temp_train.shape)\n",
    "        final_list.append([temp_train, temp_label])\n",
    "        \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "109576a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_train = make_series(train_sc)\n",
    "series_test = make_series(test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "014b32d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1709, 600, 18)\n",
      "(1709, 600, 18)\n",
      "(1709, 600, 18)\n",
      "(1708, 600, 18)\n",
      "(1708, 600, 18)\n",
      "(1708, 600, 18)\n",
      "(1708, 600, 18)\n",
      "(1708, 600, 18)\n",
      "(1708, 600, 18)\n",
      "(1708, 600, 18)\n",
      "(1708, 600, 18)\n",
      "(1708, 600, 18)\n",
      "(1708, 600, 18)\n",
      "(1708, 600, 18)\n",
      "(1708, 600, 18)\n"
     ]
    }
   ],
   "source": [
    "ids = ids_26(train_label)\n",
    "data_list = make_split_dataset(series_train, ids, train_label['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef85a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be5cc75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset과 validation set을 만들어 주는 함수\n",
    "# validation set은 shuffle 적용 x\n",
    "def make_train(series_data, labels):\n",
    "    cat_y = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((series_data, cat_y))\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000, seed=42)\n",
    "    train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return train_dataset\n",
    "\n",
    "def make_val(series_data, labels):\n",
    "    cat_y = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((series_data, cat_y))\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "    val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3205e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62b5a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 만들어 주는 함수\n",
    "# 기존 base에서 overfitting이 심해, dropout을 늘림(아직 제출은 안해봄)\n",
    "def base():\n",
    "    seed(2021)\n",
    "    tf.random.set_seed(2021)\n",
    "    model = keras.models.Sequential([\n",
    "            keras.layers.Conv1D(filters=128, kernel_size=9, padding='same', input_shape=[600, 18]),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Conv1D(filters=256, kernel_size=6, padding='same'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dropout(0.4),\n",
    "            keras.layers.Conv1D(filters=128, kernel_size=3,padding='same'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            keras.layers.GlobalAveragePooling1D(),\n",
    "            keras.layers.Dense(61, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef654218",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = base()\n",
    "test_model.load_weights('./checkpoint/best/k_fold_cnn/0_fold_cnn_weighted_ckpt.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e8f1f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 64ms/step - loss: 0.2847 - accuracy: 0.9188\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.3766 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 64ms/step - loss: 0.3283 - accuracy: 0.9022\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.3771 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 64ms/step - loss: 0.3258 - accuracy: 0.9030 0s - loss: 0.3036 \n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.3763 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 64ms/step - loss: 0.3250 - accuracy: 0.9029\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.3780 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 63ms/step - loss: 0.3249 - accuracy: 0.9029\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.3769 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 64ms/step - loss: 0.3264 - accuracy: 0.9021\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.3780 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 63ms/step - loss: 0.3251 - accuracy: 0.9029\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.3785 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 64ms/step - loss: 0.3307 - accuracy: 0.9004\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.3768 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 63ms/step - loss: 0.3247 - accuracy: 0.9029\n",
      "9/9 [==============================] - 1s 55ms/step - loss: 0.3773 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 64ms/step - loss: 0.3252 - accuracy: 0.9029\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.3771 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 64ms/step - loss: 0.3256 - accuracy: 0.9029\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.3781 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 64ms/step - loss: 0.3256 - accuracy: 0.9029\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.3768 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 65ms/step - loss: 0.3247 - accuracy: 0.9029\n",
      "9/9 [==============================] - 1s 55ms/step - loss: 0.3803 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 63ms/step - loss: 0.3285 - accuracy: 0.9021\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.3772 - accuracy: 0.9064\n",
      "19/19 [==============================] - 1s 63ms/step - loss: 0.3257 - accuracy: 0.9029\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.3774 - accuracy: 0.9064\n"
     ]
    }
   ],
   "source": [
    "for series in data_list:\n",
    "    x_train, x_val, y_train, y_val = train_test_split(series[0], series[1], train_size=0.7, stratify=series[1], random_state=42)\n",
    "    \n",
    "    train_dataset = make_train(x_train, y_train)\n",
    "    val_dataset = make_val(x_val, y_val)\n",
    "    \n",
    "    test_model.evaluate(train_dataset)\n",
    "    test_model.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ac3cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw-ai",
   "language": "python",
   "name": "kw-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
