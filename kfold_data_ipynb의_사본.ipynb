{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kfold_data.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwACCOrCrm3yIoVBQi2wbi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DonghaeSuh/kw-AI-hackathon/blob/main/kfold_data_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-smUhHPnZGO",
        "outputId": "3a7ddf3d-628c-4538-8b63-8fbfbc6ee0b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPjKxR2hngas"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/test_data')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import signal\n",
        "from tqdm import tqdm\n",
        "from numpy.fft import fft, fftshift\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qqap31On3eU"
      },
      "source": [
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V4oPd19n5LJ"
      },
      "source": [
        "train=pd.read_csv('train_features.csv')\n",
        "train_label=pd.read_csv('train_labels.csv')\n",
        "test=pd.read_csv('test_features.csv')\n",
        "sub=pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7s_espMn_ar"
      },
      "source": [
        "train['acc_Energy']=(train['acc_x']**2+train['acc_y']**2+train['acc_z']**2)**(1/3)\n",
        "test['acc_Energy']=(test['acc_x']**2+test['acc_y']**2+test['acc_z']**2)**(1/3)\n",
        "\n",
        "train['gy_Energy']=(train['gy_x']**2+train['gy_y']**2+train['gy_z']**2)**(1/3)\n",
        "test['gy_Energy']=(test['gy_x']**2+test['gy_y']**2+test['gy_z']**2)**(1/3)\n",
        "\n",
        "train['gy_acc_Energy']=((train['gy_x']-train['acc_x'])**2+(train['gy_y']-train['acc_y'])**2+(train['gy_z']-train['acc_z'])**2)**(1/3)\n",
        "test['gy_acc_Energy']=((test['gy_x']-test['acc_x'])**2+(test['gy_y']-test['acc_y'])**2+(test['gy_z']-test['acc_z'])**2)**(1/3)\n",
        "\n",
        "train[\"acc_total\"]=(train[\"acc_x\"]**2+train[\"acc_y\"]**2+train[\"acc_z\"]**2)**0.5\n",
        "test[\"acc_total\"]=(test[\"acc_x\"]**2+test[\"acc_y\"]**2+test[\"acc_z\"]**2)**0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAPjBoZ_obiL"
      },
      "source": [
        "dt=0.02 \n",
        "def jerk_signal(signal): \n",
        "        return np.array([(signal[i+1]-signal[i])/dt for i in range(len(signal)-1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMxn2a9noeYf",
        "outputId": "45dcdd89-bcbb-47a3-904b-a2086d0555e4"
      },
      "source": [
        "train_dt=[]\n",
        "for i in tqdm(train['id'].unique()):\n",
        "    temp=train.loc[train['id']==i]\n",
        "    for v in train.columns[2:]:\n",
        "        values=jerk_signal(temp[v].values)\n",
        "        values=np.insert(values,0,0)\n",
        "        temp.loc[:,v+'_dt']=values\n",
        "    train_dt.append(temp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3125/3125 [01:06<00:00, 46.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcTSQrSAof-c",
        "outputId": "02f5045c-2d44-43f8-9feb-cb6489bcbff4"
      },
      "source": [
        "test_dt=[]\n",
        "for i in tqdm(test['id'].unique()):\n",
        "    temp=test.loc[test['id']==i]\n",
        "    for v in train.columns[2:]:\n",
        "        values=jerk_signal(temp[v].values)\n",
        "        values=np.insert(values,0,0)\n",
        "        temp.loc[:,v+'_dt']=values\n",
        "    test_dt.append(temp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:14<00:00, 54.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-yQCaf4o8pU"
      },
      "source": [
        "from scipy import fftpack\n",
        "from numpy.fft import *\n",
        "\n",
        "def fourier_transform_one_signal(t_signal):\n",
        "    complex_f_signal= fftpack.fft(t_signal)\n",
        "    amplitude_f_signal=np.abs(complex_f_signal)\n",
        "    return amplitude_f_signal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDKyjcH2pY8_"
      },
      "source": [
        "train=pd.concat(train_dt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ebyex48pacm",
        "outputId": "b21b3811-47e5-4342-bac6-346cc8aa0ae6"
      },
      "source": [
        "fft=[]\n",
        "for i in tqdm(train['id'].unique()):\n",
        "    temp=train.loc[train['id']==i]\n",
        "    for i in train.columns[2:]:\n",
        "        temp[i]=fourier_transform_one_signal(temp[i].values)\n",
        "    fft.append(temp)\n",
        "train=pd.concat(fft)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3125/3125 [00:25<00:00, 124.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sydhEJ8pctP"
      },
      "source": [
        "test=pd.concat(test_dt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVrGsaG_ph7u",
        "outputId": "5f9446ae-42be-42cb-8782-64330a0782d2"
      },
      "source": [
        "fft_t=[]\n",
        "for i in tqdm(test['id'].unique()):\n",
        "    temp=test.loc[test['id']==i]\n",
        "    for i in test.columns[2:]:\n",
        "        temp[i]=fourier_transform_one_signal(temp[i].values)\n",
        "    fft_t.append(temp)\n",
        "test=pd.concat(fft_t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:04<00:00, 179.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Mhb71ipj3o"
      },
      "source": [
        "col=train.columns\n",
        "train_s=train.copy()\n",
        "test_s=test.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KajJgiE_plx5"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "train_s.iloc[:,2:]= scaler.fit_transform(train_s.iloc[:,2:])\n",
        "train_sc = pd.DataFrame(data = train_s,columns =col)\n",
        "\n",
        "test_s.iloc[:,2:]= scaler.transform(test_s.iloc[:,2:])\n",
        "test_sc = pd.DataFrame(data = test_s,columns =col)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOLI1muqpoBa"
      },
      "source": [
        "def make_dataset(data):\n",
        "  ids = data['id'].unique()\n",
        "  id_data = data.groupby('id')\n",
        "  series_data = []\n",
        "\n",
        "  for i in ids:\n",
        "    df = id_data.get_group(i)\n",
        "    df = df.drop(['id', 'time'], axis=1)\n",
        "    series_data.append(df.to_numpy())\n",
        "\n",
        "  series_data = np.array(series_data)\n",
        "  return series_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm4wkdUSpq_i"
      },
      "source": [
        "series_train = make_dataset(train_sc)\n",
        "series_test = make_dataset(test_sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRJ79Qy0ps_9",
        "outputId": "c03f70a3-e88f-4e38-b5d3-5145e28e577f"
      },
      "source": [
        "series_train.shape, series_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3125, 600, 20), (782, 600, 20))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhWbAWvqpvRN"
      },
      "source": [
        "labels = train_label['label'].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w358LuXNpydZ"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIHwWQnep0F_"
      },
      "source": [
        "def make_train(series_data, labels):\n",
        "  cat_y = tf.keras.utils.to_categorical(labels)\n",
        "\n",
        "  BATCH_SIZE = 128\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((series_data, cat_y))\n",
        "  train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000, seed=42)\n",
        "  train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return train_dataset\n",
        "\n",
        "def make_val(series_data, labels):\n",
        "  cat_y = tf.keras.utils.to_categorical(labels)\n",
        "\n",
        "  BATCH_SIZE = 128\n",
        "  val_dataset = tf.data.Dataset.from_tensor_slices((series_data, cat_y))\n",
        "  val_dataset = val_dataset.batch(BATCH_SIZE)\n",
        "  val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgOI-kZ0p19N"
      },
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGVDf9Fhp47O"
      },
      "source": [
        "def weighted_loss(labels, y_pred, beta=0.9):\n",
        "        no_samples = [12, 21, 20, 23, 35, 25, 24, 26, 97, 37, 20, 23, 12,\n",
        "                     12, 25, 25, 22, 27, 47, 20, 26, 27, 19, 20, 35, 24,\n",
        "                     1518, 34, 55, 20, 35, 20, 18, 20, 22, 30, 28, 35, 20,\n",
        "                     20, 34, 20, 20, 35, 21, 22, 20, 26, 25, 30, 37, 24,\n",
        "                     12, 13, 23, 37, 36, 20, 20, 23, 48]\n",
        "\n",
        "        weight = tf.divide(1-beta, 1-tf.pow(beta, no_samples))\n",
        "        #y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        loss = labels * K.log(y_pred) * weight\n",
        "        loss = -K.sum(loss, -1)\n",
        "        return loss\n",
        "\n",
        "# 모델을 만들어 주는 함수\n",
        "# 기존 base에서 overfitting이 심해, dropout을 늘림(아직 제출은 안해봄)\n",
        "def base():\n",
        " model = keras.models.Sequential([\n",
        "            keras.layers.Conv1D(128, 9, padding='same', input_shape=[600, 20]),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Activation('relu'),\n",
        "            keras.layers.Dropout(0.3),\n",
        "            keras.layers.Conv1D(256, 6, padding='same'),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Activation('relu'),\n",
        "            keras.layers.Dropout(0.4),\n",
        "            keras.layers.Conv1D(128, 3,padding='same'),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Activation('relu'),\n",
        "            keras.layers.Dropout(0.5),\n",
        "            keras.layers.GlobalAveragePooling1D(),\n",
        "            keras.layers.Dense(61, activation='softmax')\n",
        "  ])\n",
        " model.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        " return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vju3YPEYqMvA"
      },
      "source": [
        "# checkpoint path\n",
        "# 중간중간 모델의 weight를 저장할 경로 설정\n",
        "ckpt_name = 'k_fold_cnn_ckpt.hdf5'\n",
        "checkpoint_dir_path = 'checkpoint'\n",
        "checkpoint_path = os.path.join('checkpoint', ckpt_name)\n",
        "\n",
        "# check checkpoint paht\n",
        "# 경로가 없으면 생성함\n",
        "if not(os.path.exists(checkpoint_dir_path)):\n",
        "  os.mkdir(checkpoint_dir_path)\n",
        "\n",
        "# callback 함수 목록\n",
        "callbacks_list = [\n",
        "    # 매 epoch 마다 val_loss를 체크하여 가장 낮은 상태의 weight를 저장\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath = checkpoint_path,\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        save_weights_only=True,\n",
        "        save_best_only=True\n",
        "    ),\n",
        "    # 8번 동안 val_loss의 향상이 없으면 훈련 종료\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        verbose=1, \n",
        "        patience=8\n",
        "    )\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thpKpu1uqU4S"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLLe7jbeqWe8",
        "outputId": "7c1f29d5-a959-4d8d-883e-3449c44ef41a"
      },
      "source": [
        "# k-fold 적용\n",
        "# 데이터 수가 적어 10개의 fold 사용\n",
        "k = 10 \n",
        "split = StratifiedKFold(n_splits=k, random_state=42, shuffle=True)\n",
        "results = []    # 매 fold 훈련 후 loss 저장\n",
        "models = []     # 매 fold 훈련 후 해당 모델 저장\n",
        "\n",
        "for i, (train, val) in enumerate(split.split(series_train, labels)):\n",
        "  print('Fold', i)\n",
        "  print('#'*20)\n",
        "\n",
        "  train_dataset = make_train(series_train[train], labels[train])\n",
        "  val_dataset = make_val(series_train[val], labels[val])\n",
        "\n",
        "  model = base()\n",
        "\n",
        "  model.fit(train_dataset, validation_data=val_dataset, callbacks=callbacks_list, epochs=50)\n",
        "\n",
        "  # 모델 복원\n",
        "  model.load_weights(checkpoint_path)\n",
        "  result = model.evaluate(val_dataset)[0]\n",
        "\n",
        "  results.append(result)\n",
        "  models.append(model)\n",
        "\n",
        "  del model, train_dataset, val_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0\n",
            "####################\n",
            "Epoch 1/50\n",
            "22/22 [==============================] - 107s 5s/step - loss: 2.9014 - accuracy: 0.4417 - val_loss: 5.2570 - val_accuracy: 0.0480\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 105s 5s/step - loss: 2.1783 - accuracy: 0.5260 - val_loss: 3.0350 - val_accuracy: 0.3766\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 107s 5s/step - loss: 1.9837 - accuracy: 0.5484 - val_loss: 2.1262 - val_accuracy: 0.5295\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 1.8644 - accuracy: 0.5537 - val_loss: 1.9564 - val_accuracy: 0.5473\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 105s 5s/step - loss: 1.7489 - accuracy: 0.5761 - val_loss: 1.8356 - val_accuracy: 0.5519\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 104s 5s/step - loss: 1.6628 - accuracy: 0.5829 - val_loss: 1.8790 - val_accuracy: 0.5192\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 1.5833 - accuracy: 0.5964 - val_loss: 1.9887 - val_accuracy: 0.5220\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 104s 5s/step - loss: 1.5081 - accuracy: 0.6088 - val_loss: 1.8038 - val_accuracy: 0.5565\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 1.4388 - accuracy: 0.6348 - val_loss: 1.9521 - val_accuracy: 0.5565\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 105s 5s/step - loss: 1.3692 - accuracy: 0.6543 - val_loss: 2.2808 - val_accuracy: 0.5420\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 1.3017 - accuracy: 0.6675 - val_loss: 1.7298 - val_accuracy: 0.5711\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 105s 5s/step - loss: 1.2619 - accuracy: 0.6789 - val_loss: 1.3549 - val_accuracy: 0.6316\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 1.1937 - accuracy: 0.6988 - val_loss: 1.3027 - val_accuracy: 0.6437\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 1.1319 - accuracy: 0.7087 - val_loss: 1.1957 - val_accuracy: 0.6842\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 104s 5s/step - loss: 1.0881 - accuracy: 0.7237 - val_loss: 1.1628 - val_accuracy: 0.6949\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 1.0422 - accuracy: 0.7333 - val_loss: 1.1366 - val_accuracy: 0.7052\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 1.0097 - accuracy: 0.7500 - val_loss: 1.0241 - val_accuracy: 0.7450\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 0.9682 - accuracy: 0.7543 - val_loss: 1.0271 - val_accuracy: 0.7603\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 0.9368 - accuracy: 0.7646 - val_loss: 0.9882 - val_accuracy: 0.7436\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 104s 5s/step - loss: 0.9037 - accuracy: 0.7632 - val_loss: 0.9333 - val_accuracy: 0.7557\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 0.8610 - accuracy: 0.7795 - val_loss: 0.8770 - val_accuracy: 0.7788\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 104s 5s/step - loss: 0.8258 - accuracy: 0.7881 - val_loss: 0.8559 - val_accuracy: 0.7770\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 0.7962 - accuracy: 0.7994 - val_loss: 0.8633 - val_accuracy: 0.7838\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 0.7779 - accuracy: 0.8001 - val_loss: 0.7823 - val_accuracy: 0.7987\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 107s 5s/step - loss: 0.7388 - accuracy: 0.8112 - val_loss: 0.7699 - val_accuracy: 0.8065\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 106s 5s/step - loss: 0.7298 - accuracy: 0.8126 - val_loss: 0.7163 - val_accuracy: 0.8218\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 0.7017 - accuracy: 0.8151 - val_loss: 0.7376 - val_accuracy: 0.8076\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 104s 5s/step - loss: 0.6859 - accuracy: 0.8201 - val_loss: 0.6964 - val_accuracy: 0.8321\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 0.6585 - accuracy: 0.8243 - val_loss: 0.6688 - val_accuracy: 0.8311\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 102s 5s/step - loss: 0.6331 - accuracy: 0.8311 - val_loss: 0.6931 - val_accuracy: 0.8176\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 0.6149 - accuracy: 0.8403 - val_loss: 0.6399 - val_accuracy: 0.8421\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 0.6041 - accuracy: 0.8410 - val_loss: 0.6966 - val_accuracy: 0.8286\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 0.5802 - accuracy: 0.8492 - val_loss: 0.6702 - val_accuracy: 0.8332\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 103s 5s/step - loss: 0.5689 - accuracy: 0.8514 - val_loss: 0.6008 - val_accuracy: 0.8382\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.5506 - accuracy: 0.8553"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THe_zNmB11d0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8cLR5zE11nJ"
      },
      "source": [
        "# 결과 생성\n",
        "pred_list = []    # 예측 결과를 담을 리스트\n",
        "for model in models:\n",
        "  pred = model.predict(series_test)\n",
        "  pred_list.append(pred)\n",
        "\n",
        "pred = np.mean(pred_list, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iadthhaN17sq"
      },
      "source": [
        "# 제출물 생성\n",
        "sub.iloc[:, 1:] = pred\n",
        "sub.to_csv('submission_3.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}